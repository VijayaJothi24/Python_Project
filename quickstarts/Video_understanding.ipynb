{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VijayaJothi24/Python_Project/blob/main/quickstarts/Video_understanding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb5yiH5h8x3h"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMGdicu8PVD9"
      },
      "source": [
        "# Video understanding with Gemini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR4Ti6Q0QKIl"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Video_understanding.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w14yjWnPVD-"
      },
      "source": [
        "Gemini has from the begining been a multimodal model, capable of analyzing all sorts of medias using its [long context window](https://developers.googleblog.com/en/new-features-for-the-gemini-api-and-google-ai-studio/).\n",
        "\n",
        "[Gemini 2.0](https://ai.google.dev/gemini-api/docs/models/gemini-v2) and later bring video analysis to a whole new level as illustrated in [this video](https://www.youtube.com/watch?v=Mot-JEU26GQ):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CumMaR-sts53",
        "outputId": "c06daf7e-4ba3-4bb3-b6f1-90255cab3d0e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Mot-JEU26GQ?si=pcb7-_MZTSi_1Zkw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Building with Gemini 2.0: Video understanding\n",
        "%%html\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Mot-JEU26GQ?si=pcb7-_MZTSi_1Zkw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jexx9acnuDsA"
      },
      "source": [
        "This notebook will show you how to easily use Gemini to perform the same kind of video analysis. Each of them has different prompts that you can select using the dropdown, also feel free to experiment with your own.\n",
        "\n",
        "You can also check the [live demo](https://aistudio.google.com/starter-apps/video) and try it on your own videos on [AI Studio](https://aistudio.google.com/starter-apps/video)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0HWzIEAQYqz"
      },
      "source": [
        "## Setup\n",
        "\n",
        "This section install the SDK, set it up using your [API key](../quickstarts/Authentication.ipynb), imports the relevant libs, downloads the sample videos and upload them to Gemini.\n",
        "\n",
        "Expand the section if you are curious, but you can also just run it (it should take a couple of minutes since there are large files) and go straight to the examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzBKAaL4QYq0"
      },
      "source": [
        "### Install SDK\n",
        "\n",
        "The new **[Google Gen AI SDK](https://ai.google.dev/gemini-api/docs/sdks)** provides programmatic access to Gemini 2.0 (and previous models) using both the [Google AI for Developers](https://ai.google.dev/gemini-api/docs) and [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview) APIs. With a few exceptions, code that runs on one platform will run on both. This means that you can prototype an application using the Developer API and then migrate the application to Vertex AI without rewriting your code.\n",
        "\n",
        "More details about this new SDK on the [documentation](https://ai.google.dev/gemini-api/docs/sdks) or in the [Getting started](../quickstarts/Get_started.ipynb) notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IbKkL5ksQYq1",
        "outputId": "4c1c18e2-c165-4c51-a91f-1792f7ac38ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -U -q 'google-genai'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDUGen_kQYq2"
      },
      "source": [
        "### Setup your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](../quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0H_lRdlrQYq3"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3Lez1vBQYq3"
      },
      "source": [
        "### Initialize SDK client\n",
        "\n",
        "With the new SDK you now only need to initialize a client with you API key (or OAuth if using [Vertex AI](https://cloud.google.com/vertex-ai)). The model is now set in each call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X3CAp9YrQYq4"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITgsQyaXQYq4"
      },
      "source": [
        "### Select the Gemini model\n",
        "\n",
        "Video understanding works best Gemini 2.5 pro model. You can also select former models to compare their behavior but it is recommended to use at least the 2.0 ones.\n",
        "\n",
        "For more information about all Gemini models, check the [documentation](https://ai.google.dev/gemini-api/docs/models/gemini) for extended information on each of them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IO7IoqbrQYq5"
      },
      "outputs": [],
      "source": [
        "model_name = \"gemini-2.5-pro-exp-03-25\" # @param [\"gemini-1.5-flash-latest\",\"gemini-2.0-flash-lite\",\"gemini-2.0-flash\",\"gemini-2.5-pro-exp-03-25\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv8ULT0lvJ47"
      },
      "source": [
        "### Get sample videos\n",
        "\n",
        "You will start with uploaded videos, as it's a more common use-case, but you will also see later that you can also use Youtube videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fMcwUw48vL1N"
      },
      "outputs": [],
      "source": [
        "# Load sample images\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/videos/Pottery.mp4 -O Pottery.mp4 -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/videos/Jukin_Trailcam_Videounderstanding.mp4 -O Trailcam.mp4 -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/videos/post_its.mp4 -O Post_its.mp4 -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/videos/user_study.mp4 -O User_study.mp4 -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4YMNQulz_yY"
      },
      "source": [
        "### Upload the videos\n",
        "\n",
        "Upload all the videos using the File API. You can find modre details about how to use it in the [Get Started](../quickstarts/Get_started.ipynb#scrollTo=KdUjkIQP-G_i) notebook.\n",
        "\n",
        "This can take a couple of minutes as the videos will need to be processed and tokenized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LUUMJ4kE0OZS",
        "outputId": "815db204-9a3c-4569-f3a2-306bec490f89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for video to be processed.\n",
            "Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/u7sxx2ablrih\n",
            "Waiting for video to be processed.\n",
            "Waiting for video to be processed.\n",
            "Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/lqf6rhsj7jiq\n",
            "Waiting for video to be processed.\n",
            "Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/2ydqyj1y8vv6\n",
            "Waiting for video to be processed.\n",
            "Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/jjzhwgjzvmlq\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def upload_video(video_file_name):\n",
        "  video_file = client.files.upload(file=video_file_name)\n",
        "\n",
        "  while video_file.state == \"PROCESSING\":\n",
        "      print('Waiting for video to be processed.')\n",
        "      time.sleep(10)\n",
        "      video_file = client.files.get(name=video_file.name)\n",
        "\n",
        "  if video_file.state == \"FAILED\":\n",
        "    raise ValueError(video_file.state)\n",
        "  print(f'Video processing complete: ' + video_file.uri)\n",
        "\n",
        "  return video_file\n",
        "\n",
        "pottery_video = upload_video('Pottery.mp4')\n",
        "trailcam_video = upload_video('Trailcam.mp4')\n",
        "post_its_video = upload_video('Post_its.mp4')\n",
        "user_study_video = upload_video('User_study.mp4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF5tDbb-Q0oc"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "B0Z9QzC3Q2wX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from PIL import Image\n",
        "from IPython.display import display, Markdown, HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAa7sCD7tuMW"
      },
      "source": [
        "# Search within videos\n",
        "\n",
        "First, try using the model to search within your videos and describe all the animal sightings in the trailcam video.\n",
        "\n",
        "<video controls width=\"500\"><source src=\"https://storage.googleapis.com/generativeai-downloads/videos/Jukin_Trailcam_Videounderstanding.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PZw41-lsKKMf",
        "outputId": "0392a938-4460-4238-cac2-69329b2dc500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```json\n[\n  {\"timecode\": [0, 0], \"caption\": \"\\\"Grrr\\\"\"},\n  {\"timecode\": [0, 1], \"caption\": \"In a rocky, leaf-strewn wooded area during the day, a gray fox walks into view from the right.\"},\n  {\"timecode\": [0, 5], \"caption\": \"A second gray fox follows the first, sniffing the ground.\"},\n  {\"timecode\": [0, 11], \"caption\": \"The first fox jumps onto a large boulder in the center.\"},\n  {\"timecode\": [0, 17], \"caption\": \"Now seen in infrared black and white, a mountain lion walks through the same wooded area, sniffing the ground intently.\"},\n  {\"timecode\": [0, 29], \"caption\": \"The mountain lion stops, shakes its head, looks around, and continues sniffing before walking out of frame to the right.\"},\n  {\"timecode\": [0, 35], \"caption\": \"In infrared footage at night, two gray foxes are foraging on the ground near a tree.\"},\n  {\"timecode\": [0, 40], \"caption\": \"One fox lies down and rolls playfully on its back.\"},\n  {\"timecode\": [0, 44], \"caption\": \"The standing fox approaches the other, and they briefly tussle before the standing one leaps away.\"},\n  {\"timecode\": [0, 48], \"caption\": \"The fox runs towards the camera, knocking it over.\"},\n  {\"timecode\": [0, 50], \"caption\": \"The view is restored. It's still night, and infrared shows three gray foxes interacting near the rocks.\"},\n  {\"timecode\": [0, 55], \"caption\": \"One fox jumps up onto a rock.\"},\n  {\"timecode\": [1, 1], \"caption\": \"Another fox runs towards the camera, again knocking it over.\"},\n  {\"timecode\": [1, 5], \"caption\": \"In infrared at night, a mountain lion stands near the rocks, looking up the slope.\"},\n  {\"timecode\": [1, 8], \"caption\": \"It turns and walks away up the rocky slope.\"},\n  {\"timecode\": [1, 18], \"caption\": \"At night, infrared shows a mountain lion cub appearing on the rocks, followed by an adult mountain lion walking past the camera.\"},\n  {\"timecode\": [1, 29], \"caption\": \"Infrared at night shows a bobcat standing near a tree, looking towards the camera.\"},\n  {\"timecode\": [1, 32], \"caption\": \"The bobcat lowers its head to sniff the ground.\"},\n  {\"timecode\": [1, 41], \"caption\": \"It lifts its head, looks around, then walks back slightly before pausing again.\"},\n  {\"timecode\": [1, 51], \"caption\": \"During the day, a large black bear stands in the wooded area, facing the camera.\"},\n  {\"timecode\": [1, 53], \"caption\": \"The bear turns and walks away to the right.\"},\n  {\"timecode\": [1, 57], \"caption\": \"In infrared, a mountain lion walks from left to right past the camera.\"},\n  {\"timecode\": [2, 4], \"caption\": \"During the day, the rear end of a small black bear cub is seen walking away from the camera before disappearing behind bushes.\"},\n  {\"timecode\": [2, 12], \"caption\": \"A black bear cub forages on the ground.\"},\n  {\"timecode\": [2, 14], \"caption\": \"A larger black bear approaches from behind it.\"},\n  {\"timecode\": [2, 17], \"caption\": \"The two bears forage close together before walking away.\"},\n  {\"timecode\": [2, 23], \"caption\": \"At night, infrared shows a gray fox standing on a ridge overlooking distant city lights.\"},\n  {\"timecode\": [2, 25], \"caption\": \"The fox lowers its head to sniff the ground.\"},\n  {\"timecode\": [2, 34], \"caption\": \"A large black bear walks past the camera from right to left.\"},\n  {\"timecode\": [2, 42], \"caption\": \"In infrared at night, a mountain lion walks along the same ridge, overlooking the city lights, and moves out of frame.\"},\n  {\"timecode\": [2, 51], \"caption\": \"Infrared at night shows a mountain lion backing up to a tree and scent marking it by spraying.\"},\n  {\"timecode\": [2, 55], \"caption\": \"The mountain lion turns and sniffs the ground near the base of the tree.\"},\n  {\"timecode\": [3, 5], \"caption\": \"During the day, an adult black bear stands in the wooded area, looking towards the camera.\"},\n  {\"timecode\": [3, 11], \"caption\": \"It turns its head, looking around, and opens and closes its mouth, making jaw-popping sounds.\"},\n  {\"timecode\": [3, 19], \"caption\": \"The bear walks towards the camera, sniffing the ground.\"},\n  {\"timecode\": [3, 22], \"caption\": \"A light-brown colored black bear cub stands in the woods.\"},\n  {\"timecode\": [3, 26], \"caption\": \"It lowers its head and begins foraging on the ground.\"},\n  {\"timecode\": [3, 30], \"caption\": \"Another, slightly darker cub approaches from the right.\"},\n  {\"timecode\": [3, 32], \"caption\": \"The first cub looks up briefly before continuing to forage.\"},\n  {\"timecode\": [3, 40], \"caption\": \"The second cub walks past the camera as a third, darker cub approaches the first.\"},\n  {\"timecode\": [3, 44], \"caption\": \"The two cubs forage near each other.\"},\n  {\"timecode\": [3, 50], \"caption\": \"One cub sits down and scratches its side with its hind leg.\"},\n  {\"timecode\": [3, 57], \"caption\": \"The third, darker cub walks past the sitting cub.\"},\n  {\"timecode\": [4, 1], \"caption\": \"The two lighter cubs follow the darker one, walking away from the camera.\"},\n  {\"timecode\": [4, 22], \"caption\": \"In infrared at night, a bobcat sits in a clearing, looking at the camera.\"},\n  {\"timecode\": [4, 24], \"caption\": \"The bobcat gets up, turns, and walks away over a fallen log.\"},\n  {\"timecode\": [4, 29], \"caption\": \"Infrared at night shows a gray fox appearing from the left.\"},\n  {\"timecode\": [4, 33], \"caption\": \"It pauses, looking directly at the camera.\"},\n  {\"timecode\": [4, 36], \"caption\": \"The fox turns slightly, then looks back at the camera before walking away over the log.\"},\n  {\"timecode\": [4, 44], \"caption\": \"In infrared at night, a gray fox appears, looking at the camera.\"},\n  {\"timecode\": [4, 47], \"caption\": \"It suddenly turns and runs away quickly, followed briefly by the rear end of another fox.\"},\n  {\"timecode\": [4, 57], \"caption\": \"At night, infrared shows a mountain lion sniffing the ground near the base of a tree.\"},\n  {\"timecode\": [5, 4], \"caption\": \"The mountain lion looks up briefly, then turns and walks away to the right.\"}\n]\n```"
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "prompt = \"For each scene in this video, generate captions that describe the scene along with any spoken text placed in quotation marks. Place each caption into an object with the timecode of the caption in the video.\"  # @param [\"For each scene in this video, generate captions that describe the scene along with any spoken text placed in quotation marks. Place each caption into an object with the timecode of the caption in the video.\", \"Organize all scenes from this video in a table, along with timecode, a short description, a list of objects visible in the scene (with representative emojis) and an estimation of the level of excitement on a scale of 1 to 10\"] {\"allow-input\":true}\n",
        "\n",
        "video = trailcam_video # @param [\"trailcam_video\", \"pottery_video\", \"post_its_video\", \"user_study_video\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=[\n",
        "        video,\n",
        "        prompt,\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOQzKYGJKAnD"
      },
      "source": [
        "The prompt used is quite a generic one, but you can get even better results if you cutomize it to your needs (like asking specifically for foxes).\n",
        "\n",
        "The [live demo on AI Studio](https://aistudio.google.com/starter-apps/video) shows how you can postprocess this output to jump directly to the the specific part of the video by clicking on the timecodes. If you are interested, you can check the [code of that demo on Github](https://github.com/google-gemini/starter-applets/tree/main/video)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wog32E7CKnT6"
      },
      "source": [
        "# Extract and organize text\n",
        "\n",
        "Gemini can also read what's in the video and extract it in an organized way. You can even use Gemini reasoning capabilities to generate new ideas for you.\n",
        "\n",
        "<video controls width=\"400\"><source src=\"https://storage.googleapis.com/generativeai-downloads/videos/post_its.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "baNCeA3GKrfu",
        "outputId": "17935ab5-7d13-46e0-b1e6-f8ed7856f0a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here are the transcribed project names from the sticky notes in the video, organized into a table, followed by a few additional ideas based on the existing themes.\n\n**Transcribed Project Names from Sticky Notes**\n\n| Project Name Ideas         |\n| :------------------------- |\n| Brainstorm: Project Name (Title on board) |\n| Convergence              |\n| Supernova Echo           |\n| Chaos Field              |\n| Prometheus Rising        |\n| Lunar Eclipse            |\n| Astral Forge             |\n| Draco                    |\n| Lynx                     |\n| Chimera Dream            |\n| Galactic Core            |\n| Canis Major              |\n| Comet's Tail             |\n| Delphinus                |\n| Perseus Shield           |\n| Euler's Path             |\n| Zephyr                   |\n| Titan                    |\n| Stellar Nexus            |\n| Centaurus                |\n| Serpens                  |\n| Equilibrium              |\n| Chaos Theory             |\n| Echo                     |\n| Odin                     |\n| Leo Minor                |\n| Andromeda's Reach        |\n| Orion's Belt             |\n| Symmetry                 |\n| Golden Ratio             |\n| Athena's Eye             |\n| Phoenix                  |\n| Aether                   |\n|                           |\n| Bayes Theorem            |\n| Lyra                     |\n| Fractal                  |\n| Infinity Loop            |\n| Medusa                   |\n| Hera                     |\n| Athena                   |\n| Cerberus                 |\n| Riemann's Hypothesis     |\n| Chaos Theory (appears again) |\n| Taylor Series            |\n| Stokes Theorem           |\n| Orion's Sword            |\n| Vector                   |\n| Sagitta                  |\n| Pandora's Box            |\n|                           |\n| Celestial Drift          |\n| *Possibly others obscured/partially visible* |\n\n**Additional Brainstormed Ideas (Based on Themes)**\n\nHere are a few more ideas, following the patterns of astronomy, mythology, math/physics concepts, and abstract terms seen on the board:\n\n1.  **Event Horizon:** (Astronomy/Physics) - The boundary around a black hole beyond which nothing can escape.\n2.  **Nebula:** (Astronomy) - An interstellar cloud of dust, hydrogen, helium and other ionized gases.\n3.  **Möbius Strip:** (Mathematics/Topology) - A surface with only one side and only one boundary.\n4.  **Quantum Leap:** (Physics/Figurative) - An abrupt change or significant advance.\n5.  **Icarus Flight:** (Mythology) - Referencing ambition and potential risk.\n6.  **Cygnus X-1:** (Astronomy) - A well-known galactic X-ray source, believed to be a black hole.\n7.  **Oracle Engine:** (Mythology/Technology) - Suggests prediction or deep insight.\n8.  **Lagrange Point:** (Celestial Mechanics) - Points in space where gravitational forces produce enhanced regions of equilibrium."
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "prompt = \"Transcribe the sticky notes, organize them and put it in a table. Can you come up with a few more ideas?\" # @param [\"Transcribe the sticky notes, organize them and put it in a table. Can you come up with a few more ideas?\", \"Which of those names who fit an AI product that can resolve complex questions using its thinking abilities?\"] {\"allow-input\":true}\n",
        "\n",
        "video = post_its_video # @param [\"trailcam_video\", \"pottery_video\", \"post_its_video\", \"user_study_video\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=[\n",
        "        video,\n",
        "        prompt,\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjKIsLDMTNk1"
      },
      "source": [
        "# Structure information\n",
        "\n",
        "Gemini 2.0 is not only able to read text but also to reason and structure about real world objects. Like in this video about a display of ceramics with handwritten prices and notes.\n",
        "\n",
        "<video controls width=\"500\"><source src=\"https://storage.googleapis.com/generativeai-downloads/videos/Pottery.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bqzqedMFT5Wp",
        "outputId": "2c8a337d-29a9-438e-eab5-eca5de221c10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here are the items and their associated notes from the image, presented in a table.\n\n| Item        | Notes/Details                                  | Price   |\n| :---------- | :--------------------------------------------- | :------ |\n| Tumblers    | #5 Artichoke double dip, 4\"h x 3\"d -ish         | \\$20    |\n| Small bowls | 3.5\"h x 6.5\"d                                  | \\$35    |\n| Med bowls   | 4\"h x 7\"d                                      | \\$40    |\n| Glaze Note  | #6 gemini double dip, SLOW COOL (Test tile shown) | N/A     |"
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "prompt = \"Give me a table of my items and notes\" # @param [\"Give me a table of my items and notes\", \"Help me come up with a selling pitch for my potteries\"] {\"allow-input\":true}\n",
        "\n",
        "video = pottery_video # @param [\"trailcam_video\", \"pottery_video\", \"post_its_video\", \"user_study_video\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=[\n",
        "        video,\n",
        "        prompt,\n",
        "    ],\n",
        "    config = types.GenerateContentConfig(\n",
        "        system_instruction=\"Don't forget to escape the dollar signs\",\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsh6i-Z6VHNK"
      },
      "source": [
        "As you can see, Gemini is able to grasp to with item corresponds each note, including the last one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIfsFC0pVUTD"
      },
      "source": [
        "# Analyze screen recordings for key moments\n",
        "\n",
        "You can also use the model to analyze screen recordings. Let's say you're doing user studies on how people use your product, so you end up with lots of screen recordings, like this one, that you have to manually comb through.\n",
        "With just one prompt, the model can describe all the actions in your video.\n",
        "\n",
        "<video controls width=\"400\"><source src=\"https://storage.googleapis.com/generativeai-downloads/videos/user_study.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wrMHZ0MxW75y",
        "outputId": "b4030e19-1773-4d16-9383-55a81b292d1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here is a summary of the video:\n\n(00:00-00:12) The video opens on a mobile app interface called \"My Garden App,\" displaying a list of plants like Rose, Fern, Cactus, and Monstera, each with a description, price, a \"Like\" button, and an \"Add to Cart\" button. The user begins by clicking the \"Like\" button for the Rose Plant and then the Fern.\n(00:13-00:18) The user then adds the Fern to the cart by clicking the \"Add to Cart\" button, which temporarily changes to \"Added!\". They proceed to like the Cactus and then add it to the cart as well.\n(00:22-00:35) After scrolling down, the user likes the Hibiscus plant and adds it to the cart. They navigate using the bottom tabs, first viewing the \"Cart\" which lists the Fern, Cactus, and Hibiscus with a total price, and then the \"Profile\" page showing counts for liked plants and cart items.\n(00:36-00:48) Returning to the \"Home\" screen, the user unlikes the Hibiscus, scrolls down, likes the Snake Plant, and adds the Orchid to the cart."
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "prompt = \"Generate a paragraph that summarizes this video. Keep it to 3 to 5 sentences with corresponding timecodes.\" # @param [\"Generate a paragraph that summarizes this video. Keep it to 3 to 5 sentences with corresponding timecodes.\", \"Choose 5 key shots from this video and put them in a table with the timecode, text description of 10 words or less, and a list of objects visible in the scene (with representative emojis).\", \"Generate bullet points for the video. Place each bullet point into an object with the timecode of the bullet point in the video.\"] {\"allow-input\":true}\n",
        "\n",
        "video = user_study_video # @param [\"trailcam_video\", \"pottery_video\", \"post_its_video\", \"user_study_video\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=[\n",
        "        video,\n",
        "        prompt,\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEYYemjyKcZ7"
      },
      "source": [
        "# Analyze youtube videos\n",
        "\n",
        "On top of using your own videos you can also ask Gemini to get a video from Youtube and analyze it. He's an example using the keynote from Google IO 2023. Guess what the main theme was?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DP0Dd0hJKvYm",
        "outputId": "77c70dd6-106b-4300-ff3a-9612ae3475a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here are all the instances where Sundar Pichai says \"AI\" during his presentation, along with timestamps and context:\n\n1.  **0:29** - \"**AI** is having a very busy year.\"\n    *   **Context:** Sundar is welcoming the audience and setting the stage for the keynote, acknowledging the significant recent activity and public interest in Artificial Intelligence.\n2.  **0:38** - \"Seven years into our journey as an **AI**-first company...\"\n    *   **Context:** He's reflecting on Google's long-term strategic commitment to integrating AI across the company, positioning them at an \"exciting inflection point.\"\n3.  **0:45** - \"...make **AI** even more helpful for people, for businesses, for communities, for everyone.\"\n    *   **Context:** Sundar is describing the opportunity presented by the current advancements in AI to make it broadly beneficial and useful.\n4.  **0:54** - \"We've been applying **AI** to make our products radically more helpful for a while.\"\n    *   **Context:** He's referencing Google's history of using AI to improve its existing product offerings, leading into the discussion of generative AI.\n5.  **1:40** - \"...more advanced writing features powered by **AI**.\"\n    *   **Context:** Sundar is discussing the evolution of Gmail features, explaining that Smart Compose led to even more sophisticated AI-driven writing assistance within Google Workspace.\n6.  **3:02** - \"...Since the early days of Street View, **AI** has stitched together billions of panoramic images...\"\n    *   **Context:** He is introducing Google Maps improvements and explaining how AI was fundamental in creating the Street View experience from the beginning.\n7.  **3:14** - \"...Immersive View, which uses **AI** to create a high-fidelity representation of a place...\"\n    *   **Context:** Sundar is describing the technology behind the Immersive View feature in Google Maps, highlighting AI's role in generating detailed 3D models.\n8.  **5:08** - \"Another product made better by **AI** is Google Photos.\"\n    *   **Context:** He transitions to discussing Google Photos, explicitly stating it's another example of a product enhanced by AI.\n9.  **5:15** - \"...It was one of our first **AI**-native products.\"\n    *   **Context:** Sundar is emphasizing the deep integration and foundation of AI within Google Photos since its inception in 2015.\n10. **5:38** - \"**AI** advancements give us more powerful ways to do this.\"\n    *   **Context:** He's explaining that progress in AI enables Google to create more powerful photo editing tools within Google Photos.\n11. **5:47** - \"...uses **AI**-powered computational photography to remove unwanted distractions.\"\n    *   **Context:** Sundar is describing how the Magic Eraser feature in Google Photos works, attributing its capabilities to AI and computational photography.\n12. **5:58** - \"...semantic understanding and generative **AI**, you can do much more...\"\n    *   **Context:** He is introducing the upcoming Magic Editor feature, explaining that it leverages a combination of different AI techniques, including generative AI.\n13. **7:40** - \"...examples of how **AI** can help you in moments that matter.\"\n    *   **Context:** Sundar is summarizing the previously shown examples (Gmail, Maps, Photos) to illustrate the practical helpfulness of AI in everyday situations.\n14. **7:47** - \"...deliver the full potential of **AI** across the products you know and love.\"\n    *   **Context:** He is expressing Google's broader ambition to continue integrating and leveraging AI capabilities throughout its product suite.\n15. **8:23** - \"...making **AI** helpful for everyone is the most profound way we will advance our mission.\"\n    *   **Context:** Sundar is connecting Google's focus on AI directly to its core mission, positioning helpful AI as the key driver for future progress.\n16. **8:53** - \"...building and deploying **AI** responsibly so that everyone can benefit equally.\"\n    *   **Context:** He is outlining the fourth key aspect of Google's approach to AI, emphasizing the commitment to responsible and ethical development and deployment.\n17. **9:02** - \"...make **AI** helpful for everyone relies on continuously advancing our foundation models.\"\n    *   **Context:** Sundar is linking the goal of creating helpful AI applications to the necessity of improving the underlying large-scale AI models (foundation models).\n18. **11:27** - \"It uses **AI** to better detect malicious scripts...\"\n    *   **Context:** He is describing Sec-PaLM, a fine-tuned version of their PaLM model, explaining its specific application of AI in identifying security threats.\n19. **12:15** - \"You can imagine an **AI** collaborator that helps radiologists interpret images...\"\n    *   **Context:** Sundar is illustrating a potential future use case for Med-PaLM 2, showcasing how AI could assist medical professionals.\n20. **12:46** - \"...journey to bring **AI** in responsible ways to billions of people.\"\n    *   **Context:** He is framing the development of PaLM 2 as part of Google's ongoing, long-term effort to make AI accessible and beneficial globally, while emphasizing responsibility.\n21. **12:57** - \"...defining **AI** breakthroughs over the last decade...\"\n    *   **Context:** Sundar is reflecting on the history of AI innovation at Google, crediting the Brain and DeepMind teams for major advancements.\n22. **14:09** - \"...deeply investing in **AI** responsibility.\"\n    *   **Context:** He is reiterating Google's commitment to safety and ethical considerations as they develop increasingly powerful AI models like Gemini.\n23. **15:04** - \"...every one of our **AI**-generated images has that metadata.\"\n    *   **Context:** Sundar is stating Google's policy to include metadata in images created by their AI, ensuring transparency about the origin of the content.\n24. **15:11** - \"...our responsible approach to **AI** later.\"\n    *   **Context:** He is mentioning that James Manyika will delve deeper into Google's responsible AI practices later in the keynote.\n25. **15:29** - \"...our experiment for conversational **AI**.\"\n    *   **Context:** Sundar is defining Google Bard and its purpose as an experimental platform for exploring and developing conversational AI interfaces."
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=types.Content(\n",
        "        parts=[\n",
        "            types.Part(text=\"Find all the instances where Sundar says \\\"AI\\\". Provide timestamps and broader context for each instance.\"),\n",
        "            types.Part(\n",
        "                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=ixRanV-rdAQ')\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cizoUEdIYLd0"
      },
      "source": [
        "Once again, you can check the  [live demo on AI Studio](https://aistudio.google.com/starter-apps/video) shows an example on how to postprocess this output. Check the [code of that demo](https://github.com/google-gemini/starter-applets/tree/main/video) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lND4jB6MrsSk"
      },
      "source": [
        "# Next Steps\n",
        "\n",
        "Try with you own videos using the [AI Studio's live demo](https://aistudio.google.com/starter-apps/video) or play with the examples from this notebook (in case you haven't seen, there are other prompts you can try in the dropdowns).\n",
        "\n",
        "For more examples of the Gemini 2.0 capabilities, check the [Gemini 2.0 folder of the cookbook](https://github.com/google-gemini/cookbook/tree/main/gemini-2/). You'll learn how to use the [Live API](../quickstarts/Get_started_LiveAPI.ipynb), juggle with [multiple tools](../quickstarts/Get_started_LiveAPI_tools.ipynb) or use Gemini 2.0 [spatial understanding](../quickstarts/Spatial_understanding.ipynb) abilities.\n",
        "\n",
        "The [examples](https://github.com/google-gemini/cookbook/tree/main/examples/) folder from the cookbook is also full of nice code samples illustrating creative ways to use Gemini multimodal capabilities and long-context."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Video_understanding.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}